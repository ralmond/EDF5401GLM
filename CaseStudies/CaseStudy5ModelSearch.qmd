---
title: "Case Study 5"
format: html
---

# Goal

What is the effect of the study conditions on the relationship between the pre-test and posttest?

# Normal Residuals

```{r}
normdat <- data.frame(x=rnorm(100),y=rnorm(100))
plot(y~x,data=normdat)
```

# A little bit of TeX (LaTeX)

$\TeX$ and $\LaTeX$

## Commands & Groups

-   `$` and `$$`
-   `\` -- starts a command
-   `{}` --- gives you a gruop

## Subscripts and superscripts

-   Subscript `_` $b_0$, $x_{ij}$
-   Superscript `^` $R^2$, $X^{-1}$

## Greek letters and other commands

Greek letters are `\` followed by the name $\theta$, $\Theta$

`\sqrt` $\sqrt{2\pi}$

Note $\log$ (in roman type)

## Sums and Products

$$\sum_{i=1}^{N} x_i$$

## Fractions

$\frac{1}{2}$

## Bold and roman

`\text` to get roman `\textbf` or `\boldsymbol` to get bold.

# Model Selection

## Maximum Likelihood

*Likelihood* is the probability of the data given the model and parameters.

$$P(\boldsymbol{Y}|\textbf{X},{\cal M},\boldsymbol{\theta}) = 
\prod P(Y_i | \boldsymbol{x}_i,{\cal M},\boldsymbol{\theta})$$ The *maximum likelihood* estimate of the parameters, $\hat{\boldsymbol{\theta}}$ is the values of the parameters that maximizes the likelihood.

Often look at the *log likelihood*

$${\cal L}(\boldsymbol{Y}|\textbf{X},{\cal M},\boldsymbol{\theta}) = 
\sum \log P(Y_i | \boldsymbol{x}_i,{\cal M},\boldsymbol{\theta})$$

For normal errors

$\log{P(Y|X,\beta) \propto (Y-\hat{Y})^2}$

For normal errors, MLE = Least Squares

## Base and Saturated Models

Base Model: Needs to have all variables related to our research question.

Null Model: Just intercept

`post_scaled ~ pre_scaled + Cond_code` (compare to without `Cond_code`)

Other variables are to soak up variance.

Maximum or Saturated Model: Model will all variables we might consider.

`names(data)`

## Forward Selection

Start with Minimum Model

Add variable with highest correlation with residuals.

Look at change in $R^2$

Stop when ~~no~~ minimal improvement.

In R, use `add1()` or `update()`

## Reverse Selection

Start with saturated model.

Drop terms with non-significant slopes.

Stop just before fit becomes noticeably worse.

In R use `drop1()` or `update()`

## Nested Models and $F$-test

Model 1 is nested in Model 2 ${\cal M}_1 \subset {\cal M}_2$ if every term in Model 1 is also in Model 2.

Difference in log likelihoods has approximately chi-squared. For normal model we can do an ANOVA $F$-test.

## Stepwise Regression

Goes forwards and backwards, adding new variables and removing old ones. Usually defines an "F to enter" and "F to leave".

# Evaluating Model Fit

```{r}
normdat <- data.frame(y=rnorm(100),x=rnorm(100),
                      x1=rnorm(100),x2=rnorm(100))
mod1 <- lm(y~x,normdat)
mod2 <- lm(y~x+x1,normdat)
mod3 <- lm(y~x+x1+x2,normdat)
```

## Adjusting R-squared

```{r}
summary(mod1)$r.squared
summary(mod2)$r.squared
summary(mod3)$r.squared
```

```{r}
summary(mod1)$adj.r.squared
summary(mod2)$adj.r.squared
summary(mod3)$adj.r.squared
```

## Cross Validation

Split data into *training* and *test* data.

Do model search on training data

Do hypothesis testing of test data.

$K$-fold cross validation -- break data into $K$ groups. $K$ times fit to $K-1$ groups and test on the remaining ones (average over the $K$ times).

Leave one out (LOO) -- $N$-fold cross validation.

Three stage -- Split training data into training and test groups.

## Deviance

Deviance is $-2 \log \\ \text{likelihood}=D$

## AIC

$$AIC=2p + D$$

Related to LOO
Also called Mallow's $C_p$. 

## BIC

$$BIC=p \ln(N) + D$$

Related to minimum description length.

## Box's Maxim

Box (1987). "Essentially, all models are wrong, but some are useful. However, the approximate nature of the model must always be borne in mind..."

Box (1976) "Since all models are wrong ..." "... the scientist cannot obtain the 'correct' one by excessive elaboration." "... the scientist must be alert to what is importantly wrong."

"The map is not the terrain".

## Occam's Window and Model Averaging

Adrian Raftery's idea:

Search for the best model, but keep the $k$ best models.

In Bayesian framework, can create a posterior distribution over models.

(Weighted) Average of predictions is better than 
prediction from any single model.

# ACED model for non-control students

```{r}
library(tidyverse)
library(DescTools)
library(GGally)
library(plotly)
```

# ACED Data

```{r loadACEDnoNA}
ACEDextract <- read_csv("ACED_extract1.csv",na="-999")
ACEDextract$Session <- factor(ACEDextract$Session)
ACEDextract$Cond_code <- factor(ACEDextract$Cond_code)
ACEDextract$Sequencing <- factor(ACEDextract$Sequencing)
ACEDextract$Feedback <- factor(ACEDextract$Feedback)
ACEDextract$Gender <- factor(ACEDextract$Gender)
ACEDextract$Race <- factor(ACEDextract$Race,1:8)
ACEDextract$Level_Code <- factor(ACEDextract$Level_Code)
```

Grab the non-control students

```{r}
ACEDexp <- filter(ACEDextract,Cond_code!="Control") %>%
  na.omit() %>%
  mutate(Cond_code=factor(case_match(Cond_code,
                              "adaptive_acc"~"adaptive_acc",
                              "adaptive_full"~"adaptive_full",
                              "linear_full"~"linear_full")))
summary(ACEDexp$Cond_code)
summary(ACEDexp$Race)
```

Want to collapse 1, 4, 5, & 8 into other

```{r recodeRace}
ACEDexp <- mutate(ACEDexp,
                  Race=factor(case_match(as.numeric(Race),
                                         7~"Reference",
                                         6~"Focal1",
                                         3~"Focal2",
                                         2~"Focal3",
                                         c(1,4,5,8)~"Other")))
summary(ACEDexp$Race)
```